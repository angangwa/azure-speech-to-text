{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of Speech to Text APIs in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "speech_key = os.getenv(\"SPEECH_KEY\")\n",
    "service_region = os.getenv(\"SERVICE_REGION\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognise from mic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_mic() -> speechsdk.SpeechRecognitionResult:\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    print(speech_recognition_result.text)\n",
    "    return speech_recognition_result\n",
    "\n",
    "\n",
    "speech_recognition_result = from_mic()\n",
    "\n",
    "print(json.dumps(json.loads(speech_recognition_result.json), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"../data/dummy-call-centre.wav\"\n",
    "audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "\n",
    "\n",
    "def from_file() -> speechsdk.SpeechRecognitionResult:\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    print(f\"Recognizing speech from file: {FILE_NAME}\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    return speech_recognition_result\n",
    "\n",
    "\n",
    "speech_recognition_result = from_file()\n",
    "\n",
    "print(json.dumps(json.loads(speech_recognition_result.json), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `speechsdk.SpeechRecognitionResult`\n",
    "\n",
    "SKD returns a `speechsdk.SpeechRecognitionResult` which can be used to understand and process output in various situatiions. This will be used in the next section when we perform continuous Speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_from_microphone():\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\n",
    "            \"No speech could be recognized: {}\".format(\n",
    "                speech_recognition_result.no_match_details\n",
    "            )\n",
    "        )\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "\n",
    "# Don't speak into the mic to see alternate results\n",
    "recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Speech recognition\n",
    "\n",
    "We can use `start_continuous_recognition()` and `stop_continuous_recognition()` to start recognizing Speech in the background. SDK provides _callbacks_ when data in available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "    speech_config=speech_config, audio_config=audio_config\n",
    ")\n",
    "\n",
    "\n",
    "## Callback function that is called each time a speech recognition event occurs\n",
    "def process_callback(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        # Print final recognised text\n",
    "        print(\"Recognised: \", evt.result.text)\n",
    "    elif evt.result.reason == speechsdk.ResultReason.RecognizingSpeech:\n",
    "        # Continuously print recognised text\n",
    "        print(\"Recognising: \", evt.result.text, end=\"\\r\")\n",
    "    else:\n",
    "        print(\"Event: {}\".format(evt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the same callback funcation for each kind of event\n",
    "#   The most interestng events are RecognizingSpeech and RecognizedSpeech.\n",
    "#   RecognizingSpeech is called when the speech recognizer has hypothesized a partial recognition result\n",
    "#   RecognizedSpeech is called when the speech recognizer has recognized a final recognition result\n",
    "speech_recognizer.recognizing.connect(process_callback)\n",
    "speech_recognizer.recognized.connect(process_callback)\n",
    "speech_recognizer.session_started.connect(process_callback)\n",
    "speech_recognizer.session_stopped.connect(process_callback)\n",
    "speech_recognizer.canceled.connect(process_callback)\n",
    "speech_recognizer.session_stopped.connect(process_callback)\n",
    "speech_recognizer.canceled.connect(process_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start continuous speech recognition\n",
    "speech_recognizer.start_continuous_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer.stop_continuous_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Continuous Speech recognition on File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"../data/dummy-call-centre.wav\"\n",
    "audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "\n",
    "\n",
    "def from_file():\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    print(f\"Recognizing speech from file: {FILE_NAME}\")\n",
    "\n",
    "    done = False\n",
    "\n",
    "    def stop_recognition(evt):\n",
    "        print(\"CLOSING on {}\".format(evt))\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    speech_recognizer.recognizing.connect(process_callback)\n",
    "    speech_recognizer.recognized.connect(process_callback)\n",
    "    speech_recognizer.session_stopped.connect(stop_recognition)\n",
    "    speech_recognizer.canceled.connect(stop_recognition)\n",
    "\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        pass\n",
    "\n",
    "\n",
    "from_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Speech recognition with diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config.set_property(\n",
    "    property_id=speechsdk.PropertyId.SpeechServiceResponse_DiarizeIntermediateResults,\n",
    "    value=\"true\",\n",
    ")\n",
    "\n",
    "\n",
    "def process_transcription_callback(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        # Print final recognised text\n",
    "        if evt.result.speaker_id:\n",
    "            print(f\"Speaker {evt.result.speaker_id}: {evt.result.text}\")\n",
    "        else:\n",
    "            print(\"Recognised: \", evt.result.text)\n",
    "    elif evt.result.reason == speechsdk.ResultReason.RecognizingSpeech:\n",
    "        # Continuously print recognised text\n",
    "        if evt.result.speaker_id:\n",
    "            print(f\"Speaker {evt.result.speaker_id}: {evt.result.text}\", end=\"\\r\")\n",
    "        else:\n",
    "            print(\"Recognising: \", evt.result.text, end=\"\\r\")\n",
    "    else:\n",
    "        print(\"Event: {}\".format(evt))\n",
    "\n",
    "\n",
    "def transcribe(file=None):\n",
    "    if file:\n",
    "        audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "    else:\n",
    "        audio_config = speechsdk.AudioConfig(use_default_microphone=True)\n",
    "    conversation_transcriber = speechsdk.transcription.ConversationTranscriber(\n",
    "        speech_config=speech_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    print(f\"Recognizing speech from file: {FILE_NAME}\")\n",
    "\n",
    "    done = False\n",
    "\n",
    "    def stop_transcription(evt):\n",
    "        print(\"CLOSING on {}\".format(evt))\n",
    "        conversation_transcriber.stop_transcribing_async()\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    conversation_transcriber.transcribing.connect(process_transcription_callback)\n",
    "    conversation_transcriber.transcribed.connect(process_transcription_callback)\n",
    "    conversation_transcriber.session_stopped.connect(stop_transcription)\n",
    "    conversation_transcriber.canceled.connect(stop_transcription)\n",
    "\n",
    "    conversation_transcriber.start_transcribing_async()\n",
    "\n",
    "    # Keep looping until keyboard interrupt\n",
    "    try:\n",
    "        while not done:\n",
    "            time.sleep(0.5)\n",
    "    except KeyboardInterrupt:\n",
    "        conversation_transcriber.stop_transcribing_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe(file=\"../data/dummy-call-centre.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://uksouth.api.cognitive.microsoft.com/speechtotext/transcriptions:transcribe?api-version=2024-11-15\"\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": speech_key}\n",
    "files = {\n",
    "    \"audio\": open(\"../data/dummy-call-centre.wav\", \"rb\"),\n",
    "    \"definition\": (None, '{\"locales\":[\"en-US\"]}'),\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, files=files)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    for phrase in response.json()[\"phrases\"]:\n",
    "        print(phrase[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    ")\n",
    "\n",
    "deployment_id = \"whisper\"  # This will correspond to the custom name you chose for your deployment when you deployed a model.\"\n",
    "audio_test_file = \"../data/dummy-call-centre.wav\"\n",
    "\n",
    "result = client.audio.transcriptions.create(\n",
    "    file=open(audio_test_file, \"rb\"), model=deployment_id\n",
    ")\n",
    "\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
