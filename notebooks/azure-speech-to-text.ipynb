{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of Speech to Text APIs in Python\n",
    "\n",
    "This notebook demonstrates various approaches for speech-to-text transcription using Azure services.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Initialization](#Setup-and-Initialization)\n",
    "2. [Azure Speech SDK](#Azure-Speech-SDK)\n",
    "   - [Recognizing from Microphone](#Recognise-from-mic)\n",
    "   - [Recognizing from File](#From-a-file)\n",
    "   - [Understanding SpeechRecognitionResult](#Understanding-speechsdk.SpeechRecognitionResult)\n",
    "3. [Continuous Speech Recognition](#Continuous-Speech-recognition)\n",
    "   - [Continuous Recognition on File](#Optional:-Continuous-Speech-recognition-on-File)\n",
    "4. [Speech Recognition with Diarization](#Continuous-Speech-recognition-with-diarization)\n",
    "5. [Fast Transcription API](#Fast-Transcription)\n",
    "6. [Azure OpenAI Whisper](#Azure-OpenAI-Whisper)\n",
    "7. [Azure OpenAI GPT-4o-transcribe](#Azure-OpenAI-GPT-4o-transcribe-Model)\n",
    "   - [File Transcription](#1.-Transcribing-Audio-Files-with-GPT-4o-transcribe)\n",
    "   - [Advanced Transcription Options](#Advanced-Options-for-File-Transcription)\n",
    "   - [Streaming Transcription for Files](#Streaming-Transcription-for-Completed-Audio-Files)\n",
    "8. [WebSockets for Real-time Transcription (OpenAI Real-time API)](#Using-WebSockets-with-OpenAI-Realtime-API-for-Live-Transcription)\n",
    "\n",
    "This notebook requires several API keys and configurations to be set in a `.env` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Initialization\n",
    "\n",
    "The following cells imports necessary libraries and environment variables for Azure Speech SDK. Additional setup will be done in the approprate sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This notebook requires several API keys and configurations to be set in a `.env` file in the same directory as the notebook. Below is a guide on how to set up your `.env` file with all required credentials.\n",
    "\n",
    "### Required Environment Variables\n",
    "\n",
    "Create a `.env` file in the notebook directory with the following variables:\n",
    "\n",
    "```\n",
    "# Azure Speech Service credentials (for Azure Speech SDK and Fast Transcription)\n",
    "SPEECH_KEY=your_azure_speech_service_key\n",
    "SERVICE_REGION=your_azure_region (e.g., uksouth, eastus)\n",
    "\n",
    "# Azure OpenAI Service credentials (for Whisper model)\n",
    "AZURE_OPENAI_API_KEY=your_azure_openai_api_key\n",
    "AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint\n",
    "\n",
    "# Azure OpenAI GPT-4o credentials (for GPT-4o-transcribe)\n",
    "AZURE_OPENAI_GPT4O_API_KEY=your_azure_openai_gpt4o_api_key\n",
    "AZURE_OPENAI_GPT4O_ENDPOINT=your_azure_openai_gpt4o_endpoint\n",
    "AZURE_OPENAI_GPT4O_DEPLOYMENT_ID=your_azure_openai_gpt4o_deployment_id\n",
    "\n",
    "# Direct OpenAI API credentials (optional, for using OpenAI's services directly)\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "```\n",
    "\n",
    "### Credential Usage\n",
    "\n",
    "| Variable | Used For |\n",
    "|---------|----------|\n",
    "| `SPEECH_KEY`, `SERVICE_REGION` | Azure Speech SDK, speech recognition, diarization, and fast transcription API |\n",
    "| `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` | Azure OpenAI Whisper model for audio transcription |\n",
    "| `AZURE_OPENAI_GPT4O_API_KEY`, `AZURE_OPENAI_GPT4O_ENDPOINT`, `AZURE_OPENAI_GPT4O_DEPLOYMENT_ID` | Azure OpenAI GPT-4o-transcribe model for high-quality transcription |\n",
    "| `OPENAI_API_KEY` | Direct access to OpenAI services (optional alternative to Azure) |\n",
    "\n",
    "### Setting Up Azure Resources\n",
    "\n",
    "1. For the Azure Speech Service, create a resource in the Azure portal and copy the key and region\n",
    "2. For Azure OpenAI, create a resource and deploy the Whisper model with your chosen deployment name\n",
    "3. For GPT-4o-transcribe, deploy the model in your Azure OpenAI resource and note the deployment ID\n",
    "\n",
    "Now let's begin by importing the necessary libraries and loading these environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "import time\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "speech_key = os.getenv(\"SPEECH_KEY\")\n",
    "service_region = os.getenv(\"SERVICE_REGION\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Speech SDK\n",
    "\n",
    "Azure Speech SDK provides a robust set of speech recognition capabilities. This section demonstrates different ways to use the SDK for transcription from various sources.\n",
    "\n",
    "The Speech SDK is a software development kit that exposes many of Azure Speech Service capabilities, allowing you to develop speech-enabled applications across multiple platforms and programming languages.\n",
    "\n",
    "[Official Documentation: Azure Speech SDK](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-sdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognise from mic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_mic() -> speechsdk.SpeechRecognitionResult:\n",
    "    \"\"\"\n",
    "    Capture speech from the microphone and perform speech recognition.\n",
    "    \n",
    "    Returns:\n",
    "        speechsdk.SpeechRecognitionResult: The recognition result object\n",
    "    \"\"\"\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    print(speech_recognition_result.text)\n",
    "    return speech_recognition_result\n",
    "\n",
    "\n",
    "speech_recognition_result = from_mic()\n",
    "\n",
    "print(json.dumps(json.loads(speech_recognition_result.json), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a file\n",
    "\n",
    "The following section demonstrates how to transcribe speech from an audio file using Azure Speech SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"../data/dummy-call-centre.wav\"\n",
    "audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "\n",
    "\n",
    "def from_file() -> speechsdk.SpeechRecognitionResult:\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    print(f\"Recognizing speech from file: {FILE_NAME}\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    return speech_recognition_result\n",
    "\n",
    "\n",
    "speech_recognition_result = from_file()\n",
    "\n",
    "print(json.dumps(json.loads(speech_recognition_result.json), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `speechsdk.SpeechRecognitionResult`\n",
    "\n",
    "SKD returns a `speechsdk.SpeechRecognitionResult` which can be used to understand and process output in various situatiions. This will be used in the next section when we perform continuous Speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_from_microphone():\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\n",
    "            \"No speech could be recognized: {}\".format(\n",
    "                speech_recognition_result.no_match_details\n",
    "            )\n",
    "        )\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "\n",
    "# Don't speak into the mic to see alternate results\n",
    "recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Speech recognition\n",
    "\n",
    "We can use `start_continuous_recognition()` and `stop_continuous_recognition()` to start recognizing Speech in the background. SDK provides _callbacks_ when data in available.\n",
    "\n",
    "Continuous speech recognition enables real-time transcription by processing speech as it's being spoken rather than waiting until the end. This is particularly useful for applications requiring live transcription or voice commands.\n",
    "\n",
    "[Official Documentation: Continuous Recognition](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-recognize-speech?pivots=programming-language-python#continuous-recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "    speech_config=speech_config, audio_config=audio_config\n",
    ")\n",
    "\n",
    "\n",
    "## Callback function that is called each time a speech recognition event occurs\n",
    "def process_callback(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        # Print final recognised text\n",
    "        print(\"Recognised: \", evt.result.text)\n",
    "    elif evt.result.reason == speechsdk.ResultReason.RecognizingSpeech:\n",
    "        # Continuously print recognised text\n",
    "        print(\"Recognising: \", evt.result.text, end=\"\\r\")\n",
    "    else:\n",
    "        print(\"Event: {}\".format(evt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the same callback funcation for each kind of event\n",
    "#   The most interestng events are RecognizingSpeech and RecognizedSpeech.\n",
    "#   RecognizingSpeech is called when the speech recognizer has hypothesized a partial recognition result\n",
    "#   RecognizedSpeech is called when the speech recognizer has recognized a final recognition result\n",
    "speech_recognizer.recognizing.connect(process_callback)\n",
    "speech_recognizer.recognized.connect(process_callback)\n",
    "speech_recognizer.session_started.connect(process_callback)\n",
    "speech_recognizer.session_stopped.connect(process_callback)\n",
    "speech_recognizer.canceled.connect(process_callback)\n",
    "speech_recognizer.session_stopped.connect(process_callback)\n",
    "speech_recognizer.canceled.connect(process_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start continuous speech recognition\n",
    "speech_recognizer.start_continuous_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer.stop_continuous_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Continuous Speech recognition on File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"../data/dummy-call-centre.wav\"\n",
    "audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "\n",
    "\n",
    "def from_file():\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    print(f\"Recognizing speech from file: {FILE_NAME}\")\n",
    "\n",
    "    done = False\n",
    "\n",
    "    def stop_recognition(evt):\n",
    "        print(\"CLOSING on {}\".format(evt))\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    speech_recognizer.recognizing.connect(process_callback)\n",
    "    speech_recognizer.recognized.connect(process_callback)\n",
    "    speech_recognizer.session_stopped.connect(stop_recognition)\n",
    "    speech_recognizer.canceled.connect(stop_recognition)\n",
    "\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        pass\n",
    "\n",
    "\n",
    "from_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Speech recognition with diarization\n",
    "\n",
    "Diarization is the process of identifying and separating different speakers in an audio recording. This feature is particularly valuable for transcribing conversations, meetings, or call center interactions where multiple speakers are involved.\n",
    "\n",
    "[Official Documentation: Speaker Recognition](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speaker-recognition-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config.set_property(\n",
    "    property_id=speechsdk.PropertyId.SpeechServiceResponse_DiarizeIntermediateResults,\n",
    "    value=\"true\",\n",
    ")\n",
    "\n",
    "\n",
    "def process_transcription_callback(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        # Print final recognised text\n",
    "        if evt.result.speaker_id:\n",
    "            print(f\"Speaker {evt.result.speaker_id}: {evt.result.text}\")\n",
    "        else:\n",
    "            print(\"Recognised: \", evt.result.text)\n",
    "    elif evt.result.reason == speechsdk.ResultReason.RecognizingSpeech:\n",
    "        # Continuously print recognised text\n",
    "        if evt.result.speaker_id:\n",
    "            print(f\"Speaker {evt.result.speaker_id}: {evt.result.text}\", end=\"\\r\")\n",
    "        else:\n",
    "            print(\"Recognising: \", evt.result.text, end=\"\\r\")\n",
    "    else:\n",
    "        print(\"Event: {}\".format(evt))\n",
    "\n",
    "\n",
    "def transcribe(file=None):\n",
    "    if file:\n",
    "        audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "    else:\n",
    "        audio_config = speechsdk.AudioConfig(use_default_microphone=True)\n",
    "    conversation_transcriber = speechsdk.transcription.ConversationTranscriber(\n",
    "        speech_config=speech_config, audio_config=audio_config\n",
    "    )\n",
    "\n",
    "    print(f\"Recognizing speech from file: {FILE_NAME}\")\n",
    "\n",
    "    done = False\n",
    "\n",
    "    def stop_transcription(evt):\n",
    "        print(\"CLOSING on {}\".format(evt))\n",
    "        conversation_transcriber.stop_transcribing_async()\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    conversation_transcriber.transcribing.connect(process_transcription_callback)\n",
    "    conversation_transcriber.transcribed.connect(process_transcription_callback)\n",
    "    conversation_transcriber.session_stopped.connect(stop_transcription)\n",
    "    conversation_transcriber.canceled.connect(stop_transcription)\n",
    "\n",
    "    conversation_transcriber.start_transcribing_async()\n",
    "\n",
    "    # Keep looping until keyboard interrupt\n",
    "    try:\n",
    "        while not done:\n",
    "            time.sleep(0.5)\n",
    "    except KeyboardInterrupt:\n",
    "        conversation_transcriber.stop_transcribing_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe(file=\"../data/dummy-call-centre.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Transcription\n",
    "\n",
    "Fast Transcription is an Azure AI Speech Service REST API designed for quick, efficient transcription of audio files. It provides a simplified workflow for batch processing without the overhead of continuous recognition, making it ideal for scenarios requiring rapid transcription of pre-recorded audio.\n",
    "\n",
    "[Official Documentation: Fast Transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/fast-transcription-create?tabs=locale-specified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://uksouth.api.cognitive.microsoft.com/speechtotext/transcriptions:transcribe?api-version=2024-11-15\"\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": speech_key}\n",
    "files = {\n",
    "    \"audio\": open(\"../data/dummy-call-centre.wav\", \"rb\"),\n",
    "    \"definition\": (None, '{\"locales\":[\"en-US\"]}'),\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, files=files)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    for phrase in response.json()[\"phrases\"]:\n",
    "        print(phrase[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI Whisper\n",
    "\n",
    "Whisper is a state-of-the-art speech recognition model from OpenAI that can transcribe audio in multiple languages. The Azure OpenAI implementation provides high-quality transcription with lower latency and better cost efficiency compared to traditional speech recognition methods.\n",
    "\n",
    "[Official Documentation: Azure OpenAI Whisper Model](https://learn.microsoft.com/en-us/azure/ai-services/openai/whisper-quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    ")\n",
    "\n",
    "deployment_id = \"whisper\"  # This will correspond to the custom name you chose for your deployment when you deployed a model.\"\n",
    "audio_test_file = \"../data/dummy-call-centre.wav\"\n",
    "\n",
    "result = client.audio.transcriptions.create(\n",
    "    file=open(audio_test_file, \"rb\"), model=deployment_id\n",
    ")\n",
    "\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI GPT-4o-transcribe Model\n",
    "\n",
    "This section demonstrates how to use the GPT-4o-transcribe model for both file transcription and real-time audio streaming using Azure OpenAI.\n",
    "\n",
    "GPT-4o-transcribe is a state-of-the-art transcription model that offers high accuracy across various languages and audio qualities. It provides advantages like improved handling of domain-specific terminology and streaming capabilities.\n",
    "\n",
    "[Official Documentation: Realtime API for speech and audio](https://learn.microsoft.com/en-us/azure/ai-services/openai/realtime-audio-quickstart).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries for GPT-4o-transcribe\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import websocket\n",
    "import base64\n",
    "import queue\n",
    "\n",
    "# Load environment variables if not already loaded\n",
    "load_dotenv()\n",
    "\n",
    "# Get GPT-4o-transcribe credentials from .env file\n",
    "AZURE_OPENAI_GPT4O_API_KEY = os.getenv(\"AZURE_OPENAI_GPT4O_API_KEY\")\n",
    "AZURE_OPENAI_GPT4O_ENDPOINT = os.getenv(\"AZURE_OPENAI_GPT4O_ENDPOINT\")\n",
    "AZURE_OPENAI_GPT4O_DEPLOYMENT_ID = os.getenv(\"AZURE_OPENAI_GPT4O_DEPLOYMENT_ID\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # Direct OpenAI API key\n",
    "\n",
    "# Initialize Azure OpenAI client for GPT-4o\n",
    "gpt4o_client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_GPT4O_API_KEY,\n",
    "    api_version=\"2025-03-01-preview\",  # Make sure to use the correct API version\n",
    "    azure_endpoint=f\"https://{AZURE_OPENAI_GPT4O_ENDPOINT.split('/openai/deployments')[0]}\"  # Base endpoint without the path\n",
    ")\n",
    "\n",
    "# Initialize direct OpenAI client\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transcribing Audio Files with GPT-4o-transcribe\n",
    "\n",
    "First, let's demonstrate how to transcribe an existing audio file using GPT-4o-transcribe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_file_with_gpt4o(file_path, response_format=\"text\"):\n",
    "    \"\"\"\n",
    "    Transcribe an audio file using Azure OpenAI's GPT-4o-transcribe model\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the audio file\n",
    "        response_format (str): Format of the response ('text' or 'json')\n",
    "    \n",
    "    Returns:\n",
    "        The transcription result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as audio_file:\n",
    "            transcription = gpt4o_client.audio.transcriptions.create(\n",
    "                model=AZURE_OPENAI_GPT4O_DEPLOYMENT_ID,\n",
    "                file=audio_file,\n",
    "                response_format=response_format\n",
    "            )\n",
    "            \n",
    "            if response_format == \"json\":\n",
    "                return transcription\n",
    "            else:\n",
    "                return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing file: {e}\")\n",
    "        raise e\n",
    "        return None\n",
    "\n",
    "# Test with an audio file\n",
    "audio_test_file = \"../data/realistic-call-centre.wav\"\n",
    "\n",
    "print(\"Transcribing audio file...\")\n",
    "transcription = transcribe_file_with_gpt4o(audio_test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTranscription:\")\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Options for File Transcription\n",
    "\n",
    "GPT-4o-transcribe supports additional options like prompting to improve the quality of transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_file_with_prompt(file_path, prompt=\"\", response_format=\"text\"):\n",
    "    \"\"\"\n",
    "    Transcribe an audio file with a prompt to guide the transcription\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the audio file\n",
    "        prompt (str): A prompt to guide the transcription\n",
    "        response_format (str): Format of the response ('text' or 'json')\n",
    "    \n",
    "    Returns:\n",
    "        The transcription result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as audio_file:\n",
    "            transcription = gpt4o_client.audio.transcriptions.create(\n",
    "                model=AZURE_OPENAI_GPT4O_DEPLOYMENT_ID,\n",
    "                file=audio_file,\n",
    "                response_format=response_format,\n",
    "                prompt=prompt\n",
    "            )\n",
    "            \n",
    "            if response_format == \"json\":\n",
    "                return transcription\n",
    "            else:\n",
    "                return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with a prompt for call center context\n",
    "call_center_prompt = \"The following is a call center conversation between a customer service representative and a customer discussing a banking issue.\"\n",
    "\n",
    "print(\"Transcribing audio file with call center context prompt...\")\n",
    "transcription_with_prompt = transcribe_file_with_prompt(\n",
    "    audio_test_file, \n",
    "    prompt=call_center_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTranscription with prompt:\")\n",
    "print(transcription_with_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Transcription for Completed Audio Files\n",
    "\n",
    "GPT-4o-transcribe supports streaming responses for completed audio files, which allows getting transcription results incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_transcription_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Stream transcription results from a completed audio file\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the audio file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as audio_file:\n",
    "            stream = gpt4o_client.audio.transcriptions.create(\n",
    "                model=AZURE_OPENAI_GPT4O_DEPLOYMENT_ID,\n",
    "                file=audio_file,\n",
    "                response_format=\"json\",\n",
    "                stream=True,\n",
    "                include=[\"logprobs\"],\n",
    "            )\n",
    "            \n",
    "            full_transcript = \"\"\n",
    "            print(\"Streaming transcription:\")\n",
    "            for event in stream:\n",
    "                if event.type == \"transcript.text.delta\":\n",
    "                    full_transcript += event.delta\n",
    "                    full_transcript = full_transcript.replace(\"\\n\", \"\")\n",
    "                    print(\"Recognizing: \", full_transcript, end=\"\\r\", flush=True)\n",
    "                elif event.type == \"transcript.text.done\":\n",
    "                    return event            \n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error streaming transcription: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming transcription from file...\")\n",
    "transcipt_response = stream_transcription_from_file(audio_test_file)\n",
    "\n",
    "print(\"\\n\\nFinal Transcription:\")\n",
    "print(transcipt_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each token and its log probability\n",
    "# for logprob in transcipt_response.logprobs:\n",
    "#     token = logprob.token\n",
    "#     logprob = logprob.logprob\n",
    "#     print(f\"Token: {token}, Log Probability: {logprob}\")\n",
    "    \n",
    "# Example output:\n",
    "# Token:  on, Log Probability: -5.9153886e-06\n",
    "# Token:  you, Log Probability: -0.0067254375\n",
    "# Token: ., Log Probability: -0.0021892798\n",
    "# Token:  Wow, Log Probability: -4.441817e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the transcription result, color coded by log probability\n",
    "def color_gradient(value, min_value, max_value):\n",
    "    \"\"\"\n",
    "    Generate a color gradient where higher values are green and lower values are red\n",
    "    \n",
    "    Args:\n",
    "        value (float): The value to color\n",
    "        min_value (float): The minimum value for the gradient\n",
    "        max_value (float): The maximum value for the gradient\n",
    "    \n",
    "    Returns:\n",
    "        str: ANSI escape code for the color\n",
    "    \"\"\"\n",
    "    ratio = (value - min_value) / (max_value - min_value)\n",
    "    g = int(255 * ratio)  # Green increases with value\n",
    "    r = int(255 * (1 - ratio))  # Red decreases with value\n",
    "    return f\"\\033[38;2;{r};{g};0m\"  # RGB color code\n",
    "\n",
    "# Print the transcription result with a color gradient based on log probability\n",
    "for logprob in transcipt_response.logprobs:\n",
    "    token = logprob.token\n",
    "    logprob = logprob.logprob\n",
    "    prob = np.round(np.exp(logprob) * 100, 2)\n",
    "    color = color_gradient(prob, 0, 100)  # Color gradient from 0 to 100%\n",
    "    print(f\"{color}{token}\\033[0m\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using WebSockets with OpenAI Realtime API for Live Transcription\n",
    "\n",
    "This section demonstrates how to use WebSockets for real-time audio transcription using OpenAI's Realtime API. This API allows for continuous audio streaming and transcription, which is useful for applications like voice assistants, live captioning, and more.\n",
    "\n",
    "### How WebSocket Transcription Works\n",
    "\n",
    "The WebSocket-based transcription service provides several advantages over traditional file-based transcription:\n",
    "\n",
    "1. **Real-time results**: Transcription happens as you speak, without waiting for the complete audio\n",
    "2. **Continuous streaming**: Audio is sent in small chunks through a persistent connection\n",
    "3. **Turn detection**: Automatically detects speech segments using Voice Activity Detection (VAD)\n",
    "4. **Configurable noise reduction**: Can be optimized for near-field or far-field speech\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "The `TranscriptionService` class in `transcription_websocket_service.py` supports these key parameters:\n",
    "\n",
    "- `service_type`: Choose between `\"azure\"` or `\"openai\"` (direct) services\n",
    "- `model`: Specify model (\"gpt-4o-transcribe\" or \"gpt-4o-mini-transcribe\")\n",
    "- `noise_reduction`: Set to \"near_field\" or \"far_field\" for different environments\n",
    "- `turn_threshold`: Sensitivity for voice activity detection (0.0-1.0)\n",
    "- `include_logprobs`: Whether to include confidence scores for transcribed text\n",
    "\n",
    "### Required Environment Variables\n",
    "\n",
    "For Azure OpenAI service (used in this example):\n",
    "- `AZURE_OPENAI_GPT4O_ENDPOINT`: Your Azure OpenAI endpoint\n",
    "- `AZURE_OPENAI_GPT4O_DEPLOYMENT_ID`: The deployment name for your GPT-4o-transcribe model\n",
    "- `AZURE_OPENAI_GPT4O_API_KEY`: Your Azure OpenAI API key\n",
    "\n",
    "For direct OpenAI service (alternative option):\n",
    "- `OPENAI_API_KEY`: Your OpenAI API key\n",
    "\n",
    "### Official Documentation\n",
    "\n",
    "- [OpenAI Speech to Text Documentation](https://platform.openai.com/docs/guides/speech-to-text)\n",
    "- [OpenAI Realtime Transcription Guide](https://platform.openai.com/docs/guides/realtime-transcription)\n",
    "- [Realtime Transcription API Reference](https://platform.openai.com/docs/guides/realtime-transcription#page-top)\n",
    "\n",
    "Note: The WebSocket API is currently in preview as of May 2025. Refer to [`transcription_websocket_service.py`](./transcription_websocket_service.py) for detailed implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "from transcription_websocket_service import start_azure_transcription\n",
    "\n",
    "# Enable asyncio in Jupyter only if needed\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.environ.get(\"AZURE_OPENAI_GPT4O_ENDPOINT\")\n",
    "deployment = os.environ.get(\"AZURE_OPENAI_GPT4O_DEPLOYMENT_ID\")\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_GPT4O_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript, probs = start_azure_transcription(\n",
    "    endpoint=endpoint, \n",
    "    deployment=deployment, \n",
    "    api_key=api_key, \n",
    "    duration=60\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
